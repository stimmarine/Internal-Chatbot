{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["itQNjNYcW3by","22Zo8TxbbkT3","TURN0EoNnZxp","U0vGvSYKXahJ"],"toc_visible":true,"authorship_tag":"ABX9TyPNum2Bzd/40h5OtR8eEg10"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Phase 1"],"metadata":{"id":"DZ9ZMDUvbrUo"}},{"cell_type":"markdown","source":["## General setup - run everytime"],"metadata":{"id":"NSeZs4PKWt32"}},{"cell_type":"code","source":["!pip install google-generativeai streamlit faiss-cpu python-docx tiktoken PyMuPDF pyngrok pandas docx2txt \\\n","  \"llama-index>=0.10\" llama-index-llms-google-genai llama-index-embeddings-google-genai llama-index-vector-stores-faiss \\\n","  -q"],"metadata":{"id":"BIqabmncCXrq","executionInfo":{"status":"ok","timestamp":1747037599836,"user_tz":-60,"elapsed":26030,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"34202a89-09af-4603-e323-77cf9eb87e7e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ME0zPA-G6tm9","executionInfo":{"status":"ok","timestamp":1747037726107,"user_tz":-60,"elapsed":126260,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"4876d5c8-c117-4678-e6c1-7f840c689a65"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot\n"]}],"source":["from google.colab import drive\n","import os\n","from google.colab import userdata\n","import google.generativeai as genai\n","\n","drive.mount('/content/drive')\n","PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot' # Choose your project directory name\n","os.chdir(PROJECT_DIR) # Navigate to your project directory (optional, but good for relative paths later)\n","print(f\"Current working directory: {os.getcwd()}\")"]},{"cell_type":"code","source":["# --- Retrieve API key: Gemini version ---\n","try:\n","    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n","    if GEMINI_API_KEY is None:\n","        raise ValueError(\"Gemini API key not found in Colab Secrets. Please ensure it's added correctly with the name GEMINI_API_KEY.\")\n","    genai.configure(api_key=GEMINI_API_KEY)\n","    print(\"Gemini API Key configured successfully.\")\n","except Exception as e:\n","    print(f\"Error configuring Gemini API Key: {e}\")\n","    print(\"Please ensure you have added your GEMINI_API_KEY to Colab Secrets (Key icon on the left).\")\n","\n","if 'GEMINI_API_KEY' in locals() and GEMINI_API_KEY: # Ensure GEMINI_API_KEY was loaded\n","    os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY\n","    print(\"✅ GOOGLE_API_KEY environment variable set.\")\n","else:\n","    print(\"❌ GEMINI_API_KEY not found, so GOOGLE_API_KEY environment variable was not set.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdhfIUx-7qhj","executionInfo":{"status":"ok","timestamp":1747037726848,"user_tz":-60,"elapsed":733,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"8944afdb-d831-41e5-9c13-34fa7492126a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Gemini API Key configured successfully.\n","✅ GOOGLE_API_KEY environment variable set.\n"]}]},{"cell_type":"markdown","source":["## 1-time setup and test - not needed anymore"],"metadata":{"id":"itQNjNYcW3by"}},{"cell_type":"code","source":["if 'GEMINI_API_KEY' in locals() and GEMINI_API_KEY: # Check if API key was loaded and configured\n","    try:\n","        # For text-only input, use a Gemini model like \"gemini-1.5-flash-latest\" or \"gemini-pro\"\n","        # model = genai.GenerativeModel('gemini-pro')\n","        model = genai.GenerativeModel('gemini-1.5-flash-latest') # Often good for quick tasks and cost-effective\n","\n","        prompt = \"What are three emerging trends in sustainable packaging for 2025, according to your knowledge cutoff?\"\n","        response = model.generate_content(prompt)\n","\n","        print(\"API Call Successful!\")\n","        print(\"Response from Gemini:\")\n","        # print(response.text) # Access the text directly\n","        # Or, to handle potential blocks if any (though for simple text usually not an issue)\n","        if response.parts:\n","            print(response.parts[0].text)\n","        else:\n","            print(\"No content parts found in response. Full response:\", response)\n","\n","\n","    except Exception as e:\n","        print(f\"An error occurred during the Gemini API call: {e}\")\n","        # You can print response.prompt_feedback for safety ratings if the call fails due to content policy\n","        # For example, if hasattr(response, 'prompt_feedback'): print(response.prompt_feedback)\n","else:\n","    print(\"Gemini API Key not set or configured. Please complete Step 1.2 correctly.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"KZKNl9nq8bsS","executionInfo":{"status":"ok","timestamp":1746609313275,"user_tz":-60,"elapsed":3528,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"e7f25e7a-b381-4a9f-dba8-e65e75fe4517"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["API Call Successful!\n","Response from Gemini:\n","My knowledge cutoff is 2021, so I can't predict specific emerging trends *for* 2025. However, based on the trends observable in 2021, I can extrapolate three likely emerging trends in sustainable packaging by then:\n","\n","1. **Increased use of bio-based and compostable materials:**  The movement away from petroleum-based plastics was already accelerating in 2021. By 2025, we would likely see a wider adoption and refinement of bioplastics derived from renewable sources like seaweed, mushrooms, and agricultural waste.  Improved compostability and biodegradability, alongside reduced cost and improved performance characteristics, would be key factors driving this trend.\n","\n","2. **Focus on packaging reduction and reuse systems:**  Minimizing packaging material would continue to be a priority.  This would manifest in lighter-weight packaging designs, innovative packaging formats that reduce material use, and a strong push towards reusable packaging systems, including deposit-return schemes and subscription models for reusable containers.  The advancements in digital tracking and logistics would play a crucial role in supporting this.\n","\n","3. **Enhanced transparency and traceability through digital technologies:**  Consumers are increasingly demanding transparency about the environmental impact of products and their packaging.  This trend would likely see greater adoption of blockchain technology and other digital tools to provide detailed information about the sourcing of materials, manufacturing processes, and end-of-life management options.  This increased transparency would also help build consumer trust and drive the adoption of sustainable practices.\n","\n","\n","It's important to note that these are projections based on pre-2021 data, and the actual trends in 2025 may differ due to unforeseen technological advancements, regulatory changes, or shifts in consumer preferences.\n","\n"]}]},{"cell_type":"code","source":["# Ensure PROJECT_DIR is set from Cell 1.0 or define it here if running independently\n","\n","sub_directories = [\"data\", \"notebooks\", \"vectorstore\", \"utils\", \"app\"] # 'app' for Streamlit\n","\n","# Create a placeholder requirements.txt (we'll populate it later for deployment)\n","requirements_file = os.path.join(PROJECT_DIR, \"requirements.txt\")\n","if not os.path.exists(requirements_file):\n","    with open(requirements_file, \"w\") as f:\n","        f.write(\"# Google Generative AI and LlamaIndex\\n\")\n","        f.write(\"google-generativeai\\n\")\n","        f.write(\"llama-index>=0.10\\n\")\n","        f.write(\"faiss-cpu\\n\")\n","        f.write(\"# Document Loaders\\n\")\n","        f.write(\"pandas\\n\")\n","        f.write(\"python-docx\\n\")\n","        f.write(\"PyMuPDF\\n\")\n","        f.write(\"# Web App\\n\")\n","        f.write(\"streamlit\\n\")\n","        f.write(\"# Other\\n\")\n","        f.write(\"tiktoken\\n\") # LlamaIndex might still use this for some operations\n","    print(f\"'{requirements_file}' created.\")\n","else:\n","    print(f\"'{requirements_file}' already exists.\")\n","\n","# Create a placeholder .gitignore (useful if you use Git)\n","gitignore_file = os.path.join(PROJECT_DIR, \".gitignore\")\n","if not os.path.exists(gitignore_file):\n","    with open(gitignore_file, \"w\") as f:\n","        f.write(\"# Environment files\\n\")\n","        f.write(\".env\\n\")\n","        f.write(\"*.env\\n\")\n","        f.write(\"\\n# Colab/Jupyter specific\\n\")\n","        f.write(\".ipynb_checkpoints/\\n\")\n","        f.write(\"__pycache__/\\n\")\n","        f.write(\"\\n# Data files (if large or sensitive and you don't want to commit them)\\n\")\n","        f.write(\"# data/\\n\") # Example: uncomment to ignore data folder\n","        f.write(\"\\n# Vectorstore files\\n\")\n","        f.write(\"vectorstore/\\n\") # Usually good to ignore the built index\n","    print(f\"'{gitignore_file}' created.\")\n","else:\n","    print(f\"'{gitignore_file}' already exists.\")\n","\n","# Create a placeholder README.md\n","readme_file = os.path.join(PROJECT_DIR, \"README.md\")\n","if not os.path.exists(readme_file):\n","    with open(readme_file, \"w\") as f:\n","        f.write(\"# My Market AI Assistant Project (using Gemini)\\n\\n\")\n","        f.write(\"This project aims to build an AI assistant for market research insights, powered by Google Gemini.\\n\")\n","    print(f\"'{readme_file}' created.\")\n","else:\n","    print(f\"'{readme_file}' already exists.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qxfMhBiYBnzY","executionInfo":{"status":"ok","timestamp":1746609631653,"user_tz":-60,"elapsed":18,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"dd29be0e-204d-486b-ff65-dc101e945f3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Directory '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/data' already exists.\n","Directory '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/notebooks' already exists.\n","Directory '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore' already exists.\n","Directory '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/utils' already exists.\n","Directory '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/app' already exists.\n","'/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/requirements.txt' already exists.\n","'/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/.gitignore' already exists.\n","'/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/README.md' already exists.\n"]}]},{"cell_type":"markdown","source":["## Git commands: require expertise to run"],"metadata":{"id":"T4Q9zBQiXA61"}},{"cell_type":"code","source":["# --- Git ---\n","\n","# Configure Git (only need to do this once per Colab session if not persisted)\n","!git config --global user.name \"stimmarine\"\n","!git config --global user.email \"stevenyap2357@gmail.com\"\n","\n","# Navigate to your project directory (ensure this is correct)\n","os.chdir(PROJECT_DIR) # PROJECT_DIR should be set from Cell 1.0\n","print(f\"Current directory for Git operations: {os.getcwd()}\")\n","\n","# Initialize Git, add files, commit, and push\n","!git init\n","!git add .\n","!git commit -m \"Phase 1: Initial project setup with Gemini and Colab environment\"\n","!git remote add origin https://github.com/stimmarine/MR_Chatbot.git\n","!git branch -M main\n","!git push -u origin main\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zSp3FJNyC7wG","executionInfo":{"status":"ok","timestamp":1746612652835,"user_tz":-60,"elapsed":3386,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"0d1c5b71-762d-4503-b7cb-4bc2049f594e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current directory for Git operations: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot\n","\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n","\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n","\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n","\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit branch -m <name>\u001b[m\n","Initialized empty Git repository in /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/.git/\n","[master (root-commit) 4310bf5] Phase 1: Initial project setup with Gemini and Colab environment\n"," 4 files changed, 29 insertions(+)\n"," create mode 100644 .gitignore\n"," create mode 100644 MR_Chatbot.ipynb\n"," create mode 100644 README.md\n"," create mode 100644 requirements.txt\n","fatal: could not read Username for 'https://github.com': No such device or address\n"]}]},{"cell_type":"code","source":["# Cell 1.5.1 - Pushing to GitHub with PAT\n","\n","from google.colab import userdata\n","import os\n","\n","# --- Configuration ---\n","# Make sure PROJECT_DIR is the same as in your previous cells\n","PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot' # VERIFY THIS PATH\n","GITHUB_USERNAME = \"stimmarine\" # Your GitHub username\n","REPO_NAME = \"MR_Chatbot\"\n","\n","os.chdir(PROJECT_DIR) # Ensure we are in the correct directory\n","\n","try:\n","    GITHUB_PAT = userdata.get('GITHUB_PAT')\n","    if GITHUB_PAT is None:\n","        raise ValueError(\"GITHUB_PAT not found in Colab Secrets.\")\n","\n","    # Check current branch name\n","    current_branch_output = !git rev-parse --abbrev-ref HEAD\n","    current_local_branch = current_branch_output[0]\n","    print(f\"Current local branch is: {current_local_branch}\")\n","\n","    # If current local branch is 'master', and you want 'main' on remote (standard practice)\n","    # Ensure local 'main' branch exists and is up-to-date, or rename master to main\n","    if current_local_branch == \"master\":\n","        print(\"Local branch is 'master'. Renaming to 'main' locally.\")\n","        !git branch -M main\n","        current_local_branch = \"main\" # Update for the push command\n","\n","    print(f\"Attempting to push local branch '{current_local_branch}' to remote 'origin/{current_local_branch}'...\")\n","\n","    # Set the remote URL with authentication\n","    # This command updates the URL for the 'origin' remote you added earlier\n","    !git remote set-url origin https://{GITHUB_USERNAME}:{GITHUB_PAT}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\n","\n","    # Now, try pushing again\n","    # The -u flag sets the upstream branch so that future `git pull` and `git push` can be run without specifying origin and branch\n","    !git push -u origin {current_local_branch}\n","\n","    print(f\"\\nPush attempt finished. Check your GitHub repository: https://github.com/{GITHUB_USERNAME}/{REPO_NAME}\")\n","    print(\"If the push was successful, you should see your files there.\")\n","    print(\"Consider setting the remote URL back if you don't want the PAT stored in the .git/config long-term for this Colab runtime's Git clone:\")\n","    print(f\"# !git remote set-url origin https://github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\")\n","\n","\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")\n","    print(\"Please double-check your GITHUB_PAT in Colab Secrets, GITHUB_USERNAME, and REPO_NAME.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-AyGSAgOfz0","executionInfo":{"status":"ok","timestamp":1746613954389,"user_tz":-60,"elapsed":1849,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"f72fe09e-1535-437b-edba-d3c8afac0327"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current local branch is: main\n","Attempting to push local branch 'main' to remote 'origin/main'...\n","Enumerating objects: 6, done.\n","Counting objects: 100% (6/6), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (6/6), done.\n","Writing objects: 100% (6/6), 4.74 KiB | 285.00 KiB/s, done.\n","Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\n","To https://github.com/stimmarine/MR_Chatbot.git\n"," * [new branch]      main -> main\n","Branch 'main' set up to track remote branch 'main' from 'origin'.\n","\n","Push attempt finished. Check your GitHub repository: https://github.com/stimmarine/MR_Chatbot\n","If the push was successful, you should see your files there.\n","Consider setting the remote URL back if you don't want the PAT stored in the .git/config long-term for this Colab runtime's Git clone:\n","# !git remote set-url origin https://github.com/stimmarine/MR_Chatbot.git\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"I9rmS3kqSnOU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Phase 2"],"metadata":{"id":"z_Wn0fP1bg57"}},{"cell_type":"markdown","source":["## General setup - not needed for web-based chatbot"],"metadata":{"id":"xI-7D2eyWvAO"}},{"cell_type":"code","source":["from llama_index.llms.google_genai import GoogleGenAI as GeminiLLM # Using an alias for clarity\n","from llama_index.embeddings.google_genai import GoogleGenAIEmbedding as GeminiEmbedding # Using an alias\n","from llama_index.vector_stores.faiss import FaissVectorStore\n","from llama_index.core import Settings, VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n","\n","DATA_DIR = os.path.join(PROJECT_DIR, \"data\")\n","VECTOR_STORE_DIR = os.path.join(PROJECT_DIR, \"vectorstore\")\n","\n","# 1. Configure LlamaIndex to use Gemini\n","#    Ensure your GEMINI_API_KEY is already configured in the genai library\n","#    from the setup cell. If not, you might need to re-run that part.\n","print(\"\\n--- Configuring LlamaIndex with Gemini ---\")\n","try:\n","    # Set up the LLM (for response generation, summarization, etc.) and embedding model (this model is for Gemini)\n","    Settings.llm = GeminiLLM(model=\"models/gemini-1.5-flash-latest\")\n","    print(\"LLM configured with Gemini.\")\n","\n","    Settings.embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\") # Or \"models/embedding-001\"\n","    print(\"Embedding model configured with GeminiEmbedding.\")\n","    print(f\"Using embedding model: {Settings.embed_model.model_name}\")\n","\n","except Exception as e:\n","    print(f\"❌ Error configuring LlamaIndex with Gemini: {e}\")\n","    print(\"   Ensure your GEMINI_API_KEY and GOOGLE_API_KEY env var was set in your setup cell..\")\n","    # You might want to stop execution here if configuration fails\n","    raise"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6i3uFy-WyP1","executionInfo":{"status":"ok","timestamp":1746777976626,"user_tz":-60,"elapsed":373,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"84808e08-b508-4359-c401-336ff2ad346d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Configuring LlamaIndex with Gemini ---\n","LLM configured with Gemini.\n","Embedding model configured with GeminiEmbedding.\n","Using embedding model: models/text-embedding-004\n"]}]},{"cell_type":"markdown","source":["## 1-time setup - Load, Embed, Index, Persist Docs (rerun when docs have changed)"],"metadata":{"id":"22Zo8TxbbkT3"}},{"cell_type":"code","source":["# Load Documents\n","import os\n","import faiss # For creating the FAISS index object\n","\n","\n","# Ensure the vector store directory exists\n","if not os.path.exists(VECTOR_STORE_DIR):\n","    os.makedirs(VECTOR_STORE_DIR)\n","    print(f\"Created directory: {VECTOR_STORE_DIR}\")\n","\n","print(f\"Data directory: {DATA_DIR}\")\n","print(f\"Vector store directory: {VECTOR_STORE_DIR}\")\n","\n","# Load Documents using SimpleDirectoryReader\n","print(\"\\n--- Loading documents ---\")\n","try:\n","    if not os.listdir(DATA_DIR): # Check if the data directory is empty\n","        print(f\"⚠️ The data directory '{DATA_DIR}' is empty.\")\n","        print(\"   Please upload your sample documents (PDF, DOCX, TXT) to this folder on Google Drive.\")\n","        documents = [] # Initialize as empty list\n","    else:\n","        print(f\"Reading files from: {DATA_DIR}\")\n","        # SimpleDirectoryReader will try to read all supported files in the directory\n","        documents = SimpleDirectoryReader(DATA_DIR).load_data()\n","        if documents:\n","            print(f\"✅ Successfully loaded {len(documents)} document(s).\")\n","            for doc in documents:\n","                # Print the filename and a snippet of the text\n","                print(f\"  - Loaded: {doc.metadata.get('file_name', 'Unknown filename')}, Snippet: '{doc.text[:100].strip()}...'\")\n","        else:\n","            print(\"⚠️ No documents were loaded. Check file types and content in the data directory.\")\n","except Exception as e:\n","    print(f\"❌ Error loading documents: {e}\")\n","    documents = [] # Ensure documents is defined even if loading fails\n","\n","# Quick check\n","if not documents:\n","    print(\"\\n🔴 No documents loaded. Further steps in Phase 2 depend on having documents.\")\n","    print(\"   Please check your data directory and ensure files are uploaded correctly.\")\n","else:\n","    print(\"\\n✅ Document loading step complete.\")\n","\n","# Embed, Index, Persist Doc\n","print(f\"--- Starting Index Construction & Persistence ---\")\n","\n","if 'documents' not in locals() or not documents:\n","    print(\"🔴 No documents loaded (the 'documents' variable is empty or not defined).\")\n","    print(\"   Cannot build index. Please re-run the document loading cell (Cell 2.1) successfully first.\")\n","else:\n","    try:\n","        # 1. Determine Embedding Dimension\n","        #    We need this for initializing the FAISS index.\n","        print(\"Determining embedding dimension...\")\n","        if Settings.embed_model:\n","            # This makes a quick API call to get a sample embedding's length\n","            sample_embedding = Settings.embed_model.get_text_embedding(\"test\")\n","            d = len(sample_embedding)\n","            print(f\"✅ Detected embedding dimension: {d}\")\n","        else:\n","            # This should not happen if Cell 2.1 ran correctly\n","            print(\"⚠️ Embedding model not found in LlamaIndex Settings. Assuming dimension 768 for Gemini.\")\n","            d = 768\n","\n","        # 2. Initialize FAISS Index\n","        #    IndexFlatL2 is a common choice for exact, brute-force similarity search.\n","        faiss_index = faiss.IndexFlatL2(d)\n","        print(f\"FAISS index initialized with dimension {d}.\")\n","\n","        # 3. Create LlamaIndex FaissVectorStore\n","        #    This wraps our FAISS index for use with LlamaIndex.\n","        vector_store = FaissVectorStore(faiss_index=faiss_index)\n","        print(\"FaissVectorStore created.\")\n","\n","        # 4. Create StorageContext\n","        #    This tells LlamaIndex to use our FaissVectorStore for storing embeddings.\n","        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n","        print(\"StorageContext created using our FaissVectorStore.\")\n","\n","        # 5. Build the VectorStoreIndex\n","        #    This is the main step:\n","        #    - Documents are chunked into nodes.\n","        #    - Each node's text is embedded using Settings.embed_model (Gemini).\n","        #    - Embeddings and nodes are stored via the StorageContext (in our FAISS store).\n","        #    This can take time depending on document size/count and API latency for embeddings.\n","        print(\"⏳ Building VectorStoreIndex... This might take a few minutes...\")\n","        index = VectorStoreIndex.from_documents(\n","            documents, # Your loaded documents from Cell 2.1\n","            storage_context=storage_context,\n","            show_progress=True\n","        )\n","        print(\"✅ VectorStoreIndex built successfully.\")\n","\n","        # 6. Persist the index to disk\n","        #    This saves the LlamaIndex structures (docstore, index_store) and tells\n","        #    the FaissVectorStore to persist its own data (the FAISS index itself).\n","        print(f\"💾 Persisting index to: {VECTOR_STORE_DIR} ...\")\n","        index.storage_context.persist(persist_dir=VECTOR_STORE_DIR)\n","        print(f\"✅ Index persisted successfully to {VECTOR_STORE_DIR}\")\n","\n","    except Exception as e_build:\n","        print(f\"❌ An error occurred during index construction or persistence: {e_build}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","print(\"\\n--- Phase 2: Indexing (Build & Store) Steps Attempted ---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X4zCt5T-bi3F","executionInfo":{"status":"ok","timestamp":1746715740744,"user_tz":-60,"elapsed":189,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"99d2549d-b5d4-4b34-fff8-e9bc032aebd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data directory: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/data\n","Vector store directory: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore\n","\n","--- Loading documents ---\n","Reading files from: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/data\n","✅ Successfully loaded 5 document(s).\n","  - Loaded: ID1_PaediatricSurgeon_ShanghaiChildrensMedicalCentre_ENG_Transcript.docx, Snippet: 'Name: CJ Xie 谢晨捷\n","\n","Profession: Paediatric Surgeon\n","\n","Interview number: ID1\n","\n","Date: 16/10/2024\n","\n","City: Sha...'\n","  - Loaded: ID2_Haematology_ShanghaiChildrensMedicalCentre_ENG_Transcript.docx, Snippet: 'Name: CC Chen\n","\n","Profession: Paediatric Haematology Oncologists \n","\n","Interview number: ID2 \n","\n","Date: 18/10/...'\n","  - Loaded: ID3_Nurse_ShengjingHospital_ENG_Transcript.docx, Snippet: 'Name: HF Qu\n","\n","Profession: Paediatric Haematology Nurse \n","\n","Interview number: ID3 \n","\n","Date: 22/10/2024 \n","\n","C...'\n","  - Loaded: ID4_Haematology_ShenjingHospital_ENG_Transcript.docx, Snippet: 'Name: LC Hao\n","\n","Profession: Paediatric Haematology Oncologist \n","\n","Interview number: ID4 \n","\n","Date: 22/10/20...'\n","  - Loaded: ID5_PaediatricSurgeon_ShanghaiChildrensHospital_ENG_Transcript.docx, Snippet: 'Name: F Chen\n","\n","Profession: Paediatric Surgeon (Attending Physician 主治医生)\n","\n","Interview number: ID5\n","\n","Date...'\n","\n","✅ Document loading step complete.\n"]}]},{"cell_type":"markdown","source":["### Steven's own test"],"metadata":{"id":"TURN0EoNnZxp"}},{"cell_type":"code","source":["chat_engine = index.as_chat_engine(\n","    chat_mode=\"condense_question\", streaming=True\n",")\n","response_stream = chat_engine.stream_chat(\"Who are the moderator and interviewees\")"],"metadata":{"id":"uaUGLPgBnIX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response_stream.print_response_stream()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xHaXYX0nTVy","executionInfo":{"status":"ok","timestamp":1746719339379,"user_tz":-60,"elapsed":93,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"42bba148-61b2-4847-a710-946eaec95c52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The moderator is Steven.  The interviewees are ID4_Nurse and ID4_HaemOnco.\n"]}]},{"cell_type":"code","source":["response_stream = chat_engine.stream_chat(\"Who else?\")"],"metadata":{"id":"DNK3XUCnnijr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response_stream.print_response_stream()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXzhHbSQnlBw","executionInfo":{"status":"ok","timestamp":1746719362080,"user_tz":-60,"elapsed":81,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"b4768c60-e855-4999-bcab-11f652d5dfb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This question cannot be answered from the given source.\n"]}]},{"cell_type":"code","source":["chat_engine.reset()"],"metadata":{"id":"xKDpunNPnxGv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test/Diagnostics to check json files in vectorstore (LLM coded)"],"metadata":{"id":"U0vGvSYKXahJ"}},{"cell_type":"code","source":["# Diagnostic Cell to check persisted JSON files\n","\n","import json\n","import os\n","import glob\n","\n","# Ensure VECTOR_STORE_DIR is defined from your previous cells\n","# VECTOR_STORE_DIR = '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore' # If not defined, uncomment and set\n","\n","print(f\"--- Checking persisted JSON files in: {VECTOR_STORE_DIR} ---\")\n","\n","# Check all .json file\n","json_files_to_check = [os.path.basename(f) for f in glob.glob(os.path.join(VECTOR_STORE_DIR, \"*.json\"))]\n","print(f\"Found JSON files to check: {json_files_to_check}\")\n","\n","\n","for fname in json_files_to_check:\n","    fpath = os.path.join(VECTOR_STORE_DIR, fname)\n","    if os.path.exists(fpath):\n","        print(f\"\\nAttempting to load '{fpath}'...\")\n","        try:\n","            with open(fpath, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","            print(f\"✅ Successfully loaded '{fpath}' with utf-8 encoding.\")\n","        except UnicodeDecodeError as ude:\n","            print(f\"❌ UnicodeDecodeError when loading '{fpath}' with utf-8: {ude}\")\n","            print(\"   This file likely contains non-UTF-8 characters.\")\n","            # Optionally, try to read a few lines with a different encoding to see content (for debugging)\n","            try:\n","                with open(fpath, 'r', encoding='latin-1') as f_latin: # Try latin-1 as it's a common source of such bytes\n","                    print(f\"   First 200 chars (latin-1 decoded) of '{fpath}':\\n   '{f_latin.read(200)}'\")\n","            except:\n","                pass # Ignore if this also fails\n","        except json.JSONDecodeError as jde:\n","            print(f\"❌ JSONDecodeError when loading '{fpath}': {jde}\")\n","            print(\"   The file might be corrupted or not valid JSON.\")\n","        except Exception as e:\n","            print(f\"❌ An unexpected error occurred when loading '{fpath}': {e}\")\n","    else:\n","        print(f\"\\nFile not found (which might be okay, e.g., graph_store.json is optional): '{fpath}'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNNiBiuQSXKD","executionInfo":{"status":"ok","timestamp":1746699876653,"user_tz":-60,"elapsed":53,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"a20a6cd9-537d-4429-94d8-7f40ad61dde2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Checking persisted JSON files in: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore ---\n","Found JSON files to check: ['docstore.json', 'index_store.json', 'graph_store.json', 'default__vector_store.json', 'image__vector_store.json']\n","\n","Attempting to load '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/docstore.json'...\n","✅ Successfully loaded '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/docstore.json' with utf-8 encoding.\n","\n","Attempting to load '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/index_store.json'...\n","✅ Successfully loaded '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/index_store.json' with utf-8 encoding.\n","\n","Attempting to load '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/graph_store.json'...\n","✅ Successfully loaded '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/graph_store.json' with utf-8 encoding.\n","\n","Attempting to load '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/default__vector_store.json'...\n","❌ UnicodeDecodeError when loading '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/default__vector_store.json' with utf-8: 'utf-8' codec can't decode byte 0xc0 in position 38: invalid start byte\n","   This file likely contains non-UTF-8 characters.\n","   First 200 chars (latin-1 decoded) of '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/default__vector_store.json':\n","   'IxF2\u0000\u0003\u0000\u0000@\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0000\u0000\u0000\u0000À\u0000\u0000\u0000\u0000\u0000\u0000{=\\«£»á\u0000\u0006¼Z¿»Ø\u001cj¼xWd=\u001c§§»ö¼\u0010õ¼X(V=Éù¼#YÁ<¿à:=L\u0017¼}<i¼¹H½6M»»gÖ½ÔÏ=ºm\u0014f;Ë¿²<¹=½·½À³<;îÆF»pÂO½µ C½))</I:ÿÎA=i\"å<$\u000b^=Ä\u0017\"½¨H=¥Ò:=x¼¸\u0011=\u001ap'\n","\n","Attempting to load '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/image__vector_store.json'...\n","✅ Successfully loaded '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore/image__vector_store.json' with utf-8 encoding.\n"]}]},{"cell_type":"markdown","source":["## Load Persisted Index  "],"metadata":{"id":"DVAGmTVznbnE"}},{"cell_type":"code","source":[" # --- Optional: Test Loading the Persisted Index ---\n","print(\"\\n---  Attempting to load the persisted index  ---\")\n","# Note: Settings.llm and Settings.embed_model should still be configured globally from Cell 2.1 for the query engine to work correctly later.\n","try:\n","    print(f\"Loading index from: {VECTOR_STORE_DIR}\")\n","    vector_store = FaissVectorStore.from_persist_dir(persist_dir=VECTOR_STORE_DIR)\n","    loaded_storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=VECTOR_STORE_DIR)\n","    loaded_index = load_index_from_storage(storage_context = loaded_storage_context)\n","    print(\"✅ Index loaded successfully from disk for testing.\")\n","\n","except Exception as e_load:\n","    print(f\"❌ Error loading persisted index for testing: {e_load}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_9MKFWukFN3l","executionInfo":{"status":"ok","timestamp":1746777981508,"user_tz":-60,"elapsed":1279,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"96b3a449-ad89-4a32-a947-feef236f52aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","---  Attempting to load the persisted index  ---\n","Loading index from: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore\n","✅ Index loaded successfully from disk for testing.\n"]}]},{"cell_type":"markdown","source":["## Quick query test of loaded persisted index"],"metadata":{"id":"PSaWULqrGTwM"}},{"cell_type":"code","source":["from IPython.display import Markdown, display\n","\n","print(\"\\n--- Attempting to query loaded persisted index  ---\")\n","try:\n","    # Perform a quick test query if persisted index were loaded\n","    if loaded_index:\n","        print(\"   Performing a quick test query on the loaded index...\")\n","        query_engine = loaded_index.as_query_engine(similarity_top_k=1)\n","        response = query_engine.query(\"Who are the moderators and interviewees?\")\n","        display(Markdown(f\"<b>{response}</b>\"))\n","    else:\n","        print(\"   Skipping test query as no documents were originally loaded to build the index.\")\n","except Exception as e_load:\n","    print(f\"❌ Error loading persisted index for testing: {e_load}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98},"id":"lrYcLwdgQBV2","executionInfo":{"status":"ok","timestamp":1746778025650,"user_tz":-60,"elapsed":986,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"13a9690d-db79-45b3-97a1-198d90d0d53c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Attempting to query loaded persisted index  ---\n","   Performing a quick test query on the loaded index...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<b>The interviewer is Steven.  The interviewee is a pediatric surgeon from Shanghai Children's Medical Centre.\n</b>"},"metadata":{}}]},{"cell_type":"markdown","source":["# Phase 3"],"metadata":{"id":"-DZpbAOoGa-F"}},{"cell_type":"markdown","source":["## Setup app.py - this is the code that enables the web chatbot (run when the following code is changed)"],"metadata":{"id":"W89FoRWYJZJf"}},{"cell_type":"code","source":["# The following code when ran will be written into app.py in the project folder\n","\n","%%writefile '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/app.py'\n","# Paste the entire Streamlit app code block below this line\n","\n","import streamlit as st\n","import os\n","import faiss # For FaissVectorStore loading\n","from llama_index.core import Settings, StorageContext, load_index_from_storage\n","from llama_index.vector_stores.faiss import FaissVectorStore\n","#from llama_index.core.stores.docstore import SimpleDocumentStore #stores should be storage\n","#from llama_index.core.stores.index_store import SimpleIndexStore #stores should be storage\n","from llama_index.llms.google_genai import GoogleGenAI as GeminiLLM\n","from llama_index.embeddings.google_genai import GoogleGenAIEmbedding as GeminiEmbedding\n","import google.generativeai as genai # For API key configuration\n","\n","# --- Configuration ---\n","PROJECT_BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot'\n","VECTOR_STORE_DIR = os.path.join(PROJECT_BASE_PATH, \"vectorstore\")\n","\n","# --- LlamaIndex Setup (Cached by Streamlit) ---\n","@st.cache_resource(show_spinner=\"Initializing AI Advisor and loading knowledge base...\")\n","def load_and_setup_ai_advisor():\n","    # 1. Configure Google Gemini API Key\n","      #    Streamlit apps run in their own process. We rely on the GOOGLE_API_KEY\n","      #    environment variable being set in the environment where Streamlit runs.\n","      #    When running from Colab with `!streamlit run`, it *should* inherit.\n","    api_key = os.getenv(\"GOOGLE_API_KEY\")\n","    if not api_key:\n","        st.error(\"🔴 GOOGLE_API_KEY environment variable not found! Please ensure it's set in your Colab session before running Streamlit.\")\n","        st.stop()\n","        return None, None\n","\n","    try:\n","        genai.configure(api_key=api_key)\n","    except Exception as e:\n","        st.error(f\"🔴 Error configuring Google GenAI with API key: {e}\")\n","        st.stop()\n","        return None, None\n","\n","    # 2. Configure LlamaIndex Global Settings (LLM and Embed Model)\n","    try:\n","        Settings.llm = GeminiLLM(model=\"models/gemini-1.5-flash-latest\")\n","        Settings.embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\")\n","        st.sidebar.success(f\"LLM: {Settings.llm.model}\\nEmbed: {Settings.embed_model.model_name}\")\n","    except Exception as e:\n","        st.error(f\"🔴 Error configuring LlamaIndex Settings (LLM/Embeddings): {e}\")\n","        st.stop()\n","        return None, None\n","\n","    # 3. Load the Persisted Index\n","    try:\n","        vector_store = FaissVectorStore.from_persist_dir(persist_dir=VECTOR_STORE_DIR)\n","        storage_context = StorageContext.from_defaults(\n","            vector_store=vector_store,\n","            persist_dir=VECTOR_STORE_DIR\n","        )\n","        index = load_index_from_storage(storage_context = storage_context)\n","        # Get a chat engine\n","        # You can adjust similarity_top_k as needed. Higher means more context, potentially slower/costlier.\n","        chat_engine_obj = index.as_chat_engine(\n","            chat_mode=\"condense_question\",\n","            verbose=True,\n","            similarity_top_k=10,\n","            )\n","        st.success(\"💡 AI Advisor initialized and knowledge base loaded!\")\n","        return index, chat_engine_obj\n","    except Exception as e:\n","        st.error(f\"🔴 Error loading persisted index: {e}\")\n","        st.exception(e) # Show full traceback in Streamlit app for debugging\n","        st.stop()\n","        return None, None\n","\n","\n","# --- Streamlit App UI ---\n","st.set_page_config(page_title=\"Market AI Advisor\", page_icon=\"💡\", layout=\"wide\")\n","st.title(\"💡 Market AI Advisor\")\n","st.caption(f\"Powered by LlamaIndex and Google Gemini. Knowledge base last updated based on files in {VECTOR_STORE_DIR}\")\n","\n","# Load index and chat engine (cached)\n","# This function will only run once unless its code changes or cache is cleared.\n","loaded_index, chat_engine = load_and_setup_ai_advisor()\n","\n","if loaded_index and chat_engine:\n","    # Initialize chat history\n","    if \"messages\" not in st.session_state:\n","        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"Hello! How can I help you with your market research insights today?\"}]\n","\n","    # Display prior chat messages\n","    for message in st.session_state.messages:\n","        with st.chat_message(message[\"role\"]):\n","            st.markdown(message[\"content\"])\n","\n","    # Get new user input\n","    if prompt := st.chat_input(\"Ask your question...\"):\n","        # Add user message to session state and display it\n","        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","        with st.chat_message(\"user\"):\n","            st.markdown(prompt)\n","\n","        # Get assistant response\n","        with st.chat_message(\"assistant\"):\n","            message_placeholder = st.empty()\n","            message_placeholder.markdown(\"Thinking...\")\n","            try:\n","                response_obj = chat_engine.chat(prompt)\n","                response_text = str(response_obj)\n","\n","                # Optionally display source nodes\n","                source_nodes_md = \"\\n\\n---\\n**Retrieved Sources:**\\n\"\n","                if response_obj.source_nodes:\n","                    for i, node in enumerate(response_obj.source_nodes):\n","                        file_name = node.metadata.get('file_name', 'N/A') if node.metadata else 'N/A'\n","                        source_nodes_md += f\"{i+1}. **File:** {file_name} (Score: {node.score:.2f})\\n\"\n","                        source_nodes_md += f\"   *Snippet:* {node.text[:150].strip().replace(chr(10), ' ')}...\\n\"\n","                    response_text += source_nodes_md\n","                else:\n","                    response_text += \"\\n\\n(No specific source text segments retrieved for this query)\"\n","\n","                message_placeholder.markdown(response_text)\n","                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_text})\n","\n","            except Exception as e:\n","                error_message = f\"Sorry, an error occurred while processing your question: {e}\"\n","                st.error(error_message)\n","                st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_message})\n","else:\n","    st.info(\"AI Advisor is not ready. Please check for error messages above.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fPyYpUisYpbt","executionInfo":{"status":"ok","timestamp":1746798865318,"user_tz":-60,"elapsed":59,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"ae9c0363-0983-47b5-b7f2-e0b37d219037"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/app.py\n"]}]},{"cell_type":"markdown","source":["## Web-based Chatbot"],"metadata":{"id":"SywQOo2WkI5p"}},{"cell_type":"code","source":["from pyngrok import ngrok, conf\n","import os\n","import time\n","from datetime import datetime\n","import pytz"],"metadata":{"id":"tKw2W9ASkUEV","executionInfo":{"status":"ok","timestamp":1747040413280,"user_tz":-60,"elapsed":51,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["NGROK_AUTHTOKEN = userdata.get('NGROK_AUTHTOKEN')\n","ngrok.set_auth_token(NGROK_AUTHTOKEN)"],"metadata":{"id":"KVmbW9gXN3iv","executionInfo":{"status":"ok","timestamp":1747037808415,"user_tz":-60,"elapsed":3124,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aef26063-8570-4529-8b4c-bba86b95e4d2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":[]}]},{"cell_type":"code","source":["# Requirements\n","  # Ensure GOOGLE_API_KEY setup, i.e. it is in the environment for the Streamlit process\n","  # Set up ngrok configuration (optional, but can be useful)\n","\n","ngrok.kill() # Terminate any existing ngrok tunnels\n","app_file_path = '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/app.py'\n","public_url = ngrok.connect(8501) # Start ngrok tunnel to Streamlit's default port 8501\n","\n","# Chatbot log will be saved in the following folder with timestamp (swiss time)\n","timestamp = datetime.now(pytz.timezone('Europe/Zurich') ).strftime(\"%Y%m%d_%H%M%S\")\n","LOG_DIR = '/content/drive/MyDrive/Colab Notebooks/MR_Chatbot/chatbot logs'\n","log_file_path = os.path.join(LOG_DIR, f\"{timestamp}_streamlit.log\")\n","\n","# Run Streamlit\n","  # The `nohup` and `&` run it in the background.\n","  # We also pipe output to a log file for easier debugging if Streamlit has issues starting.\n","print(f\"Starting MR_Chatbot\")\n","!nohup streamlit run \"{app_file_path}\" --server.port 8501 &> \"{log_file_path}\" &\n","print(\"Loading... Please wait...\")\n","time.sleep(5)\n","print(\"\\nStreamlit app process has been started (or attempted).\")\n","print(f\"Access your app at: {public_url}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zpwehlVkWbNT","executionInfo":{"status":"ok","timestamp":1747040421344,"user_tz":-60,"elapsed":5875,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"7fcbebff-1958-4052-ea15-adfc048497c6"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting MR_Chatbot\n","Loading... Please wait...\n","\n","Streamlit app process has been started (or attempted).\n","Access your app at: NgrokTunnel: \"https://1174-107-167-182-207.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"markdown","source":["## Stop and disconnect"],"metadata":{"id":"XiyiU2VRJAR3"}},{"cell_type":"code","source":["print(\"Attempting to disconnect all ngrok tunnels and kill ngrok process...\")\n","try:\n","    tunnels = ngrok.get_tunnels()\n","    for tunnel in tunnels:\n","        ngrok.disconnect(tunnel.public_url)\n","        print(f\"Disconnected tunnel: {tunnel.public_url}\")\n","    ngrok.kill()\n","    print(\"ngrok process terminated.\")\n","except Exception as e:\n","    print(f\"Error stopping ngrok (it might not have been running): {e}\")\n","\n","\n","!pkill -f streamlit\n","print(\"Attempted to stop any existing Streamlit process(es).\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKOdHANrWbZu","executionInfo":{"status":"ok","timestamp":1747040296933,"user_tz":-60,"elapsed":345,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"e5ee2984-a62d-4a82-ceac-6e6a3e3bf2e3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pyngrok.process.ngrok:t=2025-05-12T08:58:16+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-938362c6-7c8d-4fc1-b0a3-7c81a639a177 acceptErr=\"failed to accept connection: Listener closed\"\n"]},{"output_type":"stream","name":"stdout","text":["Attempting to disconnect all ngrok tunnels and kill ngrok process...\n","Disconnected tunnel: https://f6c4-107-167-182-207.ngrok-free.app\n","ngrok process terminated.\n","Attempted to stop any existing Streamlit process(es).\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"uStxI7b2t1z3"},"execution_count":null,"outputs":[]}]}