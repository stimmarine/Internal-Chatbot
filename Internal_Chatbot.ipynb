{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMeghh5TUfFKdr1igqS3SfV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Step 1: Environment Setup"],"metadata":{"id":"DZ9ZMDUvbrUo"}},{"cell_type":"markdown","source":["## General setup"],"metadata":{"id":"NSeZs4PKWt32"}},{"cell_type":"code","source":["!pip install google-generativeai streamlit faiss-cpu python-docx tiktoken PyMuPDF pyngrok pandas docx2txt \\\n","  \"llama-index>=0.10\" llama-index-llms-google-genai llama-index-embeddings-google-genai llama-index-vector-stores-faiss \\\n","  -q"],"metadata":{"id":"BIqabmncCXrq","executionInfo":{"status":"ok","timestamp":1747314781298,"user_tz":-60,"elapsed":29156,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"af34380f-4d4b-4613-ac78-f38a9b38b969"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ME0zPA-G6tm9","executionInfo":{"status":"ok","timestamp":1747314845534,"user_tz":-60,"elapsed":64239,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"1b148138-ddec-42d7-daeb-4339a910041e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot\n"]}],"source":["from google.colab import drive\n","import os\n","from google.colab import userdata\n","import google.generativeai as genai\n","\n","drive.mount('/content/drive')\n","PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Internal_Chatbot' # Choose your project directory name\n","os.chdir(PROJECT_DIR) # Navigate to your project directory (optional, but good for relative paths later)\n","print(f\"Current working directory: {os.getcwd()}\")"]},{"cell_type":"code","source":["# --- Retrieve API key: Gemini version ---\n","try:\n","    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n","    if GEMINI_API_KEY is None:\n","        raise ValueError(\"Gemini API key not found in Colab Secrets. Please ensure it's added correctly with the name GEMINI_API_KEY.\")\n","    genai.configure(api_key=GEMINI_API_KEY)\n","    print(\"Gemini API Key configured successfully.\")\n","except Exception as e:\n","    print(f\"Error configuring Gemini API Key: {e}\")\n","    print(\"Please ensure you have added your GEMINI_API_KEY to Colab Secrets (Key icon on the left).\")\n","\n","if 'GEMINI_API_KEY' in locals() and GEMINI_API_KEY: # Ensure GEMINI_API_KEY was loaded\n","    os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY\n","    print(\"‚úÖ GOOGLE_API_KEY environment variable set.\")\n","else:\n","    print(\"‚ùå GEMINI_API_KEY not found, so GOOGLE_API_KEY environment variable was not set.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdhfIUx-7qhj","executionInfo":{"status":"ok","timestamp":1747314846414,"user_tz":-60,"elapsed":871,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"73c4402d-35a3-4efe-a41d-aea54efa8117"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Gemini API Key configured successfully.\n","‚úÖ GOOGLE_API_KEY environment variable set.\n"]}]},{"cell_type":"markdown","source":["# Step 2: Load Knowledge Base (rerun when docs/data have changed)"],"metadata":{"id":"22Zo8TxbbkT3"}},{"cell_type":"code","source":["# Load Documents\n","import os\n","import faiss # For creating the FAISS index object\n","\n","\n","# Ensure the vector store directory exists\n","if not os.path.exists(VECTOR_STORE_DIR):\n","    os.makedirs(VECTOR_STORE_DIR)\n","    print(f\"Created directory: {VECTOR_STORE_DIR}\")\n","\n","print(f\"Data directory: {DATA_DIR}\")\n","print(f\"Vector store directory: {VECTOR_STORE_DIR}\")\n","\n","# Load Documents using SimpleDirectoryReader\n","print(\"\\n--- Loading documents ---\")\n","try:\n","    if not os.listdir(DATA_DIR): # Check if the data directory is empty\n","        print(f\"‚ö†Ô∏è The data directory '{DATA_DIR}' is empty.\")\n","        print(\"   Please upload your sample documents (PDF, DOCX, TXT) to this folder on Google Drive.\")\n","        documents = [] # Initialize as empty list\n","    else:\n","        print(f\"Reading files from: {DATA_DIR}\")\n","        # SimpleDirectoryReader will try to read all supported files in the directory\n","        documents = SimpleDirectoryReader(DATA_DIR).load_data()\n","        if documents:\n","            print(f\"‚úÖ Successfully loaded {len(documents)} document(s).\")\n","            for doc in documents:\n","                # Print the filename and a snippet of the text\n","                print(f\"  - Loaded: {doc.metadata.get('file_name', 'Unknown filename')}, Snippet: '{doc.text[:100].strip()}...'\")\n","        else:\n","            print(\"‚ö†Ô∏è No documents were loaded. Check file types and content in the data directory.\")\n","except Exception as e:\n","    print(f\"‚ùå Error loading documents: {e}\")\n","    documents = [] # Ensure documents is defined even if loading fails\n","\n","# Quick check\n","if not documents:\n","    print(\"\\nüî¥ No documents loaded. Further steps in Phase 2 depend on having documents.\")\n","    print(\"   Please check your data directory and ensure files are uploaded correctly.\")\n","else:\n","    print(\"\\n‚úÖ Document loading step complete.\")\n","\n","# Embed, Index, Persist Doc\n","print(f\"--- Starting Index Construction & Persistence ---\")\n","\n","if 'documents' not in locals() or not documents:\n","    print(\"üî¥ No documents loaded (the 'documents' variable is empty or not defined).\")\n","    print(\"   Cannot build index. Please re-run the document loading cell (Cell 2.1) successfully first.\")\n","else:\n","    try:\n","        # 1. Determine Embedding Dimension\n","        #    We need this for initializing the FAISS index.\n","        print(\"Determining embedding dimension...\")\n","        if Settings.embed_model:\n","            # This makes a quick API call to get a sample embedding's length\n","            sample_embedding = Settings.embed_model.get_text_embedding(\"test\")\n","            d = len(sample_embedding)\n","            print(f\"‚úÖ Detected embedding dimension: {d}\")\n","        else:\n","            # This should not happen if Cell 2.1 ran correctly\n","            print(\"‚ö†Ô∏è Embedding model not found in LlamaIndex Settings. Assuming dimension 768 for Gemini.\")\n","            d = 768\n","\n","        # 2. Initialize FAISS Index\n","        #    IndexFlatL2 is a common choice for exact, brute-force similarity search.\n","        faiss_index = faiss.IndexFlatL2(d)\n","        print(f\"FAISS index initialized with dimension {d}.\")\n","\n","        # 3. Create LlamaIndex FaissVectorStore\n","        #    This wraps our FAISS index for use with LlamaIndex.\n","        vector_store = FaissVectorStore(faiss_index=faiss_index)\n","        print(\"FaissVectorStore created.\")\n","\n","        # 4. Create StorageContext\n","        #    This tells LlamaIndex to use our FaissVectorStore for storing embeddings.\n","        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n","        print(\"StorageContext created using our FaissVectorStore.\")\n","\n","        # 5. Build the VectorStoreIndex\n","        #    This is the main step:\n","        #    - Documents are chunked into nodes.\n","        #    - Each node's text is embedded using Settings.embed_model (Gemini).\n","        #    - Embeddings and nodes are stored via the StorageContext (in our FAISS store).\n","        #    This can take time depending on document size/count and API latency for embeddings.\n","        print(\"‚è≥ Building VectorStoreIndex... This might take a few minutes...\")\n","        index = VectorStoreIndex.from_documents(\n","            documents, # Your loaded documents from Cell 2.1\n","            storage_context=storage_context,\n","            show_progress=True\n","        )\n","        print(\"‚úÖ VectorStoreIndex built successfully.\")\n","\n","        # 6. Persist the index to disk\n","        #    This saves the LlamaIndex structures (docstore, index_store) and tells\n","        #    the FaissVectorStore to persist its own data (the FAISS index itself).\n","        print(f\"üíæ Persisting index to: {VECTOR_STORE_DIR} ...\")\n","        index.storage_context.persist(persist_dir=VECTOR_STORE_DIR)\n","        print(f\"‚úÖ Index persisted successfully to {VECTOR_STORE_DIR}\")\n","\n","    except Exception as e_build:\n","        print(f\"‚ùå An error occurred during index construction or persistence: {e_build}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","print(\"\\n--- Phase 2: Indexing (Build & Store) Steps Attempted ---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X4zCt5T-bi3F","executionInfo":{"status":"ok","timestamp":1746715740744,"user_tz":-60,"elapsed":189,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"99d2549d-b5d4-4b34-fff8-e9bc032aebd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data directory: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/data\n","Vector store directory: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore\n","\n","--- Loading documents ---\n","Reading files from: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/data\n","‚úÖ Successfully loaded 5 document(s).\n","  - Loaded: ID1_PaediatricSurgeon_ShanghaiChildrensMedicalCentre_ENG_Transcript.docx, Snippet: 'Name: CJ Xie Ë∞¢Êô®Êç∑\n","\n","Profession: Paediatric Surgeon\n","\n","Interview number: ID1\n","\n","Date: 16/10/2024\n","\n","City: Sha...'\n","  - Loaded: ID2_Haematology_ShanghaiChildrensMedicalCentre_ENG_Transcript.docx, Snippet: 'Name: CC Chen\n","\n","Profession: Paediatric Haematology Oncologists¬†\n","\n","Interview number: ID2¬†\n","\n","Date: 18/10/...'\n","  - Loaded: ID3_Nurse_ShengjingHospital_ENG_Transcript.docx, Snippet: 'Name: HF Qu\n","\n","Profession: Paediatric Haematology Nurse¬†\n","\n","Interview number: ID3¬†\n","\n","Date: 22/10/2024¬†\n","\n","C...'\n","  - Loaded: ID4_Haematology_ShenjingHospital_ENG_Transcript.docx, Snippet: 'Name: LC Hao\n","\n","Profession: Paediatric Haematology Oncologist¬†\n","\n","Interview number: ID4¬†\n","\n","Date: 22/10/20...'\n","  - Loaded: ID5_PaediatricSurgeon_ShanghaiChildrensHospital_ENG_Transcript.docx, Snippet: 'Name: F Chen\n","\n","Profession: Paediatric Surgeon¬†(Attending Physician ‰∏ªÊ≤ªÂåªÁîü)\n","\n","Interview number: ID5\n","\n","Date...'\n","\n","‚úÖ Document loading step complete.\n"]}]},{"cell_type":"markdown","source":["# Step 3: Setup app.py - the app.py on github should contain the following code (rerun when the following code is changed to rewrite app.py file)"],"metadata":{"id":"W89FoRWYJZJf"}},{"cell_type":"code","source":["# The following code when ran will be written into app.py in the project folder\n","\n","%%writefile '/content/drive/MyDrive/Colab Notebooks/Internal_Chatbot/app.py'\n","# Paste the entire Streamlit app code block below this line\n","\n","import streamlit as st\n","import os\n","import faiss # For FaissVectorStore loading\n","from llama_index.core import Settings, StorageContext, load_index_from_storage\n","from llama_index.vector_stores.faiss import FaissVectorStore\n","from llama_index.llms.google_genai import GoogleGenAI as GeminiLLM\n","from llama_index.embeddings.google_genai import GoogleGenAIEmbedding as GeminiEmbedding\n","import google.generativeai as genai # For API key configuration\n","\n","# --- Configuration ---\n","PROJECT_BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Internal_Chatbot'\n","VECTOR_STORE_DIR = os.path.join(PROJECT_BASE_PATH, \"vectorstore\")\n","\n","# --- LlamaIndex Setup (Cached by Streamlit) ---\n","@st.cache_resource(show_spinner=\"Initializing AI Advisor and loading knowledge base...\")\n","def load_and_setup_ai_advisor():\n","    # 1. Configure Google Gemini API Key\n","      #    Streamlit apps run in their own process. We rely on the GOOGLE_API_KEY\n","      #    environment variable being set in the environment where Streamlit runs.\n","      #    When running from Colab with `!streamlit run`, it *should* inherit.\n","    api_key = os.getenv(\"GOOGLE_API_KEY\")\n","    if not api_key:\n","        st.error(\"üî¥ GOOGLE_API_KEY environment variable not found! Please ensure it's set in your Colab session before running Streamlit.\")\n","        st.stop()\n","        return None, None\n","\n","    try:\n","        genai.configure(api_key=api_key)\n","    except Exception as e:\n","        st.error(f\"üî¥ Error configuring Google GenAI with API key: {e}\")\n","        st.stop()\n","        return None, None\n","\n","    # 2. Configure LlamaIndex Global Settings (LLM and Embed Model)\n","    try:\n","        Settings.llm = GeminiLLM(model=\"models/gemini-1.5-flash-latest\")\n","        Settings.embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\")\n","        st.sidebar.success(f\"LLM: {Settings.llm.model}\\nEmbed: {Settings.embed_model.model_name}\")\n","    except Exception as e:\n","        st.error(f\"üî¥ Error configuring LlamaIndex Settings (LLM/Embeddings): {e}\")\n","        st.stop()\n","        return None, None\n","\n","    # 3. Load the Persisted Index\n","    try:\n","        vector_store = FaissVectorStore.from_persist_dir(persist_dir=VECTOR_STORE_DIR)\n","        storage_context = StorageContext.from_defaults(\n","            vector_store=vector_store,\n","            persist_dir=VECTOR_STORE_DIR\n","        )\n","        index = load_index_from_storage(storage_context = storage_context)\n","        # Get a chat engine\n","        # You can adjust similarity_top_k as needed. Higher means more context, potentially slower/costlier.\n","        chat_engine_obj = index.as_chat_engine(\n","            chat_mode=\"condense_question\",\n","            verbose=True,\n","            similarity_top_k=10,\n","            )\n","        st.success(\"üí° AI Advisor initialized and knowledge base loaded!\")\n","        return index, chat_engine_obj\n","    except Exception as e:\n","        st.error(f\"üî¥ Error loading persisted index: {e}\")\n","        st.exception(e) # Show full traceback in Streamlit app for debugging\n","        st.stop()\n","        return None, None\n","\n","\n","# --- Streamlit App UI ---\n","st.set_page_config(page_title=\"Market AI Advisor\", page_icon=\"üí°\", layout=\"wide\")\n","st.title(\"üí° Market AI Advisor\")\n","st.caption(f\"Powered by LlamaIndex and Google Gemini. Knowledge base last updated based on files in {VECTOR_STORE_DIR}\")\n","\n","# Load index and chat engine (cached)\n","# This function will only run once unless its code changes or cache is cleared.\n","loaded_index, chat_engine = load_and_setup_ai_advisor()\n","\n","if loaded_index and chat_engine:\n","    # Initialize chat history\n","    if \"messages\" not in st.session_state:\n","        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"Hello! How can I help you with your market research insights today?\"}]\n","\n","    # Display prior chat messages\n","    for message in st.session_state.messages:\n","        with st.chat_message(message[\"role\"]):\n","            st.markdown(message[\"content\"])\n","\n","    # Get new user input\n","    if prompt := st.chat_input(\"Ask your question...\"):\n","        # Add user message to session state and display it\n","        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","        with st.chat_message(\"user\"):\n","            st.markdown(prompt)\n","\n","        # Get assistant response\n","        with st.chat_message(\"assistant\"):\n","            message_placeholder = st.empty()\n","            message_placeholder.markdown(\"Thinking...\")\n","            try:\n","                response_obj = chat_engine.chat(prompt)\n","                response_text = str(response_obj)\n","\n","                # Optionally display source nodes\n","                source_nodes_md = \"\\n\\n---\\n**Retrieved Sources:**\\n\"\n","                if response_obj.source_nodes:\n","                    for i, node in enumerate(response_obj.source_nodes):\n","                        file_name = node.metadata.get('file_name', 'N/A') if node.metadata else 'N/A'\n","                        source_nodes_md += f\"{i+1}. **File:** {file_name} (Score: {node.score:.2f})\\n\"\n","                        source_nodes_md += f\"   *Snippet:* {node.text[:150].strip().replace(chr(10), ' ')}...\\n\"\n","                    response_text += source_nodes_md\n","                else:\n","                    response_text += \"\\n\\n(No specific source text segments retrieved for this query)\"\n","\n","                message_placeholder.markdown(response_text)\n","                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_text})\n","\n","            except Exception as e:\n","                error_message = f\"Sorry, an error occurred while processing your question: {e}\"\n","                st.error(error_message)\n","                st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_message})\n","else:\n","    st.info(\"AI Advisor is not ready. Please check for error messages above.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fPyYpUisYpbt","executionInfo":{"status":"ok","timestamp":1746798865318,"user_tz":-60,"elapsed":59,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"ae9c0363-0983-47b5-b7f2-e0b37d219037"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/app.py\n"]}]},{"cell_type":"markdown","source":["# Step 4: Launch Streamlit Application"],"metadata":{"id":"-DZpbAOoGa-F"}},{"cell_type":"markdown","source":["## Web-based Chatbot"],"metadata":{"id":"SywQOo2WkI5p"}},{"cell_type":"code","source":["from pyngrok import ngrok, conf\n","import os\n","import time\n","from datetime import datetime\n","import pytz"],"metadata":{"id":"tKw2W9ASkUEV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NGROK_AUTHTOKEN = userdata.get('NGROK_AUTHTOKEN')\n","ngrok.set_auth_token(NGROK_AUTHTOKEN)"],"metadata":{"id":"KVmbW9gXN3iv","executionInfo":{"status":"ok","timestamp":1747037808415,"user_tz":-60,"elapsed":3124,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aef26063-8570-4529-8b4c-bba86b95e4d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[]}]},{"cell_type":"code","source":["# Requirements\n","  # Ensure GOOGLE_API_KEY setup, i.e. it is in the environment for the Streamlit process\n","  # Set up ngrok configuration (optional, but can be useful)\n","\n","ngrok.kill() # Terminate any existing ngrok tunnels\n","app_file_path = '/content/drive/MyDrive/Colab Notebooks/Internal_Chatbot/app.py'\n","public_url = ngrok.connect(8501) # Start ngrok tunnel to Streamlit's default port 8501\n","\n","# Chatbot log will be saved in the following folder with timestamp (swiss time)\n","timestamp = datetime.now(pytz.timezone('Europe/Zurich') ).strftime(\"%Y%m%d_%H%M%S\")\n","LOG_DIR = '/content/drive/MyDrive/Colab Notebooks/Internal_Chatbot/chatbot logs'\n","log_file_path = os.path.join(LOG_DIR, f\"{timestamp}_streamlit.log\")\n","\n","# Run Streamlit\n","  # The `nohup` and `&` run it in the background.\n","  # We also pipe output to a log file for easier debugging if Streamlit has issues starting.\n","print(f\"Starting Internal_Chatbot\")\n","!nohup streamlit run \"{app_file_path}\" --server.port 8501 &> \"{log_file_path}\" &\n","print(\"Loading... Please wait...\")\n","time.sleep(5)\n","print(\"\\nStreamlit app process has been started (or attempted).\")\n","print(f\"Access your app at: {public_url}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zpwehlVkWbNT","executionInfo":{"status":"ok","timestamp":1747040421344,"user_tz":-60,"elapsed":5875,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"7fcbebff-1958-4052-ea15-adfc048497c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting MR_Chatbot\n","Loading... Please wait...\n","\n","Streamlit app process has been started (or attempted).\n","Access your app at: NgrokTunnel: \"https://1174-107-167-182-207.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"markdown","source":["# Step 5: To stop and disconnect Chatbot"],"metadata":{"id":"XiyiU2VRJAR3"}},{"cell_type":"code","source":["print(\"Attempting to disconnect all ngrok tunnels and kill ngrok process...\")\n","try:\n","    tunnels = ngrok.get_tunnels()\n","    for tunnel in tunnels:\n","        ngrok.disconnect(tunnel.public_url)\n","        print(f\"Disconnected tunnel: {tunnel.public_url}\")\n","    ngrok.kill()\n","    print(\"ngrok process terminated.\")\n","except Exception as e:\n","    print(f\"Error stopping ngrok (it might not have been running): {e}\")\n","\n","\n","!pkill -f streamlit\n","print(\"Attempted to stop any existing Streamlit process(es).\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKOdHANrWbZu","executionInfo":{"status":"ok","timestamp":1747040296933,"user_tz":-60,"elapsed":345,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"e5ee2984-a62d-4a82-ceac-6e6a3e3bf2e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pyngrok.process.ngrok:t=2025-05-12T08:58:16+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-938362c6-7c8d-4fc1-b0a3-7c81a639a177 acceptErr=\"failed to accept connection: Listener closed\"\n"]},{"output_type":"stream","name":"stdout","text":["Attempting to disconnect all ngrok tunnels and kill ngrok process...\n","Disconnected tunnel: https://f6c4-107-167-182-207.ngrok-free.app\n","ngrok process terminated.\n","Attempted to stop any existing Streamlit process(es).\n"]}]},{"cell_type":"markdown","source":["# Extra Section: Configure LlamaIndex (For tinkering purposes in notebook, not needed for web-based chatbot)"],"metadata":{"id":"z_Wn0fP1bg57"}},{"cell_type":"markdown","source":["## General setup"],"metadata":{"id":"xI-7D2eyWvAO"}},{"cell_type":"code","source":["from llama_index.llms.google_genai import GoogleGenAI as GeminiLLM # Using an alias for clarity\n","from llama_index.embeddings.google_genai import GoogleGenAIEmbedding as GeminiEmbedding # Using an alias\n","from llama_index.vector_stores.faiss import FaissVectorStore\n","from llama_index.core import Settings, VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n","\n","DATA_DIR = os.path.join(PROJECT_DIR, \"data\")\n","VECTOR_STORE_DIR = os.path.join(PROJECT_DIR, \"vectorstore\")\n","\n","# 1. Configure LlamaIndex to use Gemini\n","#    Ensure your GEMINI_API_KEY is already configured in the genai library\n","#    from the setup cell. If not, you might need to re-run that part.\n","print(\"\\n--- Configuring LlamaIndex with Gemini ---\")\n","try:\n","    # Set up the LLM (for response generation, summarization, etc.) and embedding model (this model is for Gemini)\n","    Settings.llm = GeminiLLM(model=\"models/gemini-1.5-flash-latest\")\n","    print(\"LLM configured with Gemini.\")\n","\n","    Settings.embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\") # Or \"models/embedding-001\"\n","    print(\"Embedding model configured with GeminiEmbedding.\")\n","    print(f\"Using embedding model: {Settings.embed_model.model_name}\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Error configuring LlamaIndex with Gemini: {e}\")\n","    print(\"   Ensure your GEMINI_API_KEY and GOOGLE_API_KEY env var was set in your setup cell..\")\n","    # You might want to stop execution here if configuration fails\n","    raise"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6i3uFy-WyP1","executionInfo":{"status":"ok","timestamp":1746777976626,"user_tz":-60,"elapsed":373,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"84808e08-b508-4359-c401-336ff2ad346d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Configuring LlamaIndex with Gemini ---\n","LLM configured with Gemini.\n","Embedding model configured with GeminiEmbedding.\n","Using embedding model: models/text-embedding-004\n"]}]},{"cell_type":"markdown","source":["## Load Persisted Index  "],"metadata":{"id":"DVAGmTVznbnE"}},{"cell_type":"code","source":[" # --- Optional: Test Loading the Persisted Index ---\n","print(\"\\n---  Attempting to load the persisted index  ---\")\n","# Note: Settings.llm and Settings.embed_model should still be configured globally from Cell 2.1 for the query engine to work correctly later.\n","try:\n","    print(f\"Loading index from: {VECTOR_STORE_DIR}\")\n","    vector_store = FaissVectorStore.from_persist_dir(persist_dir=VECTOR_STORE_DIR)\n","    loaded_storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=VECTOR_STORE_DIR)\n","    loaded_index = load_index_from_storage(storage_context = loaded_storage_context)\n","    print(\"‚úÖ Index loaded successfully from disk for testing.\")\n","\n","except Exception as e_load:\n","    print(f\"‚ùå Error loading persisted index for testing: {e_load}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_9MKFWukFN3l","executionInfo":{"status":"ok","timestamp":1746777981508,"user_tz":-60,"elapsed":1279,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"96b3a449-ad89-4a32-a947-feef236f52aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","---  Attempting to load the persisted index  ---\n","Loading index from: /content/drive/MyDrive/Colab Notebooks/MR_Chatbot/vectorstore\n","‚úÖ Index loaded successfully from disk for testing.\n"]}]},{"cell_type":"markdown","source":["## chatengine test loaded persisted index"],"metadata":{"id":"TURN0EoNnZxp"}},{"cell_type":"code","source":["chat_engine = index.as_chat_engine(\n","    chat_mode=\"condense_question\", streaming=True\n",")\n","response_stream = chat_engine.stream_chat(\"Who are the moderator and interviewees\")"],"metadata":{"id":"uaUGLPgBnIX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response_stream.print_response_stream()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xHaXYX0nTVy","executionInfo":{"status":"ok","timestamp":1746719339379,"user_tz":-60,"elapsed":93,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"42bba148-61b2-4847-a710-946eaec95c52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The moderator is Steven.  The interviewees are ID4_Nurse and ID4_HaemOnco.\n"]}]},{"cell_type":"code","source":["response_stream = chat_engine.stream_chat(\"Who else?\")"],"metadata":{"id":"DNK3XUCnnijr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response_stream.print_response_stream()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXzhHbSQnlBw","executionInfo":{"status":"ok","timestamp":1746719362080,"user_tz":-60,"elapsed":81,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"b4768c60-e855-4999-bcab-11f652d5dfb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This question cannot be answered from the given source.\n"]}]},{"cell_type":"code","source":["chat_engine.reset()"],"metadata":{"id":"xKDpunNPnxGv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## queryengine test of loaded persisted index"],"metadata":{"id":"PSaWULqrGTwM"}},{"cell_type":"code","source":["from IPython.display import Markdown, display\n","\n","print(\"\\n--- Attempting to query loaded persisted index  ---\")\n","try:\n","    # Perform a quick test query if persisted index were loaded\n","    if loaded_index:\n","        print(\"   Performing a quick test query on the loaded index...\")\n","        query_engine = loaded_index.as_query_engine(similarity_top_k=1)\n","        response = query_engine.query(\"Who are the moderators and interviewees?\")\n","        display(Markdown(f\"<b>{response}</b>\"))\n","    else:\n","        print(\"   Skipping test query as no documents were originally loaded to build the index.\")\n","except Exception as e_load:\n","    print(f\"‚ùå Error loading persisted index for testing: {e_load}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98},"id":"lrYcLwdgQBV2","executionInfo":{"status":"ok","timestamp":1746778025650,"user_tz":-60,"elapsed":986,"user":{"displayName":"Steven Yap","userId":"18419937341421954449"}},"outputId":"13a9690d-db79-45b3-97a1-198d90d0d53c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Attempting to query loaded persisted index  ---\n","   Performing a quick test query on the loaded index...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<b>The interviewer is Steven.  The interviewee is a pediatric surgeon from Shanghai Children's Medical Centre.\n</b>"},"metadata":{}}]}]}